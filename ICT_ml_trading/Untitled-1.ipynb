{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04362d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:/Users/MELODY/Documents/Forex/ICT/ICT_ml_trading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32807c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cadec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e .\n",
    "\n",
    "from src.utils.visualization import plot_equity_curve, plot_drawdown, plot_metric_bar\n",
    "import pandas as pd, numpy as np\n",
    "import os, sys\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"python:\", sys.executable)\n",
    "dates = pd.date_range('2025-01-01', periods=10, freq='D')\n",
    "eq = pd.Series(1000 * (1 + np.linspace(0, 0.1, 10)), index=dates)\n",
    "plot_equity_curve(eq, show=True)\n",
    "plot_drawdown(eq, show=True)\n",
    "plot_metric_bar({'sharpe_ratio': 1.23, 'max_drawdown': -0.05}, show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/ml_models/trainer.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split, cross_val_score\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import get_scorer\n",
    "       \n",
    "\n",
    "def grid_search_with_checkpoint(\n",
    "    model: BaseEstimator,\n",
    "    param_grid: Dict[str, Any],\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    outer_splits: int = 5,\n",
    "    inner_splits: int = 3,\n",
    "    scoring: str = 'accuracy',\n",
    "    checkpoint_dir: str = 'checkpoints',\n",
    "    prefix: str = 'grid'\n",
    ") -> Tuple[BaseEstimator, float, List[float]]:\n",
    "    \"\"\"\n",
    "    üìä Nested TimeSeriesSplit CV:\n",
    "    1Ô∏è‚É£ Outer loop for honest test performance (outer_splits)\n",
    "    2Ô∏è‚É£ Inner loop for hyperparam tuning (inner_splits)\n",
    "    Returns final_model trained on full data, average outer score, and per-fold scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    outer_cv = TimeSeriesSplit(n_splits=outer_splits)\n",
    "    inner_cv = TimeSeriesSplit(n_splits=inner_splits)\n",
    "    scorer = get_scorer(scoring)\n",
    "\n",
    "    outer_scores: List[float] = []\n",
    "    best_params_list: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Outer folds\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X), start=1):\n",
    "        print(f\"üìà Outer fold {fold}/{outer_splits}\")\n",
    "        X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_te, y_te = X.iloc[test_idx],  y.iloc[test_idx]\n",
    "\n",
    "        # Inner grid search\n",
    "        best_score = float('-inf')\n",
    "        best_params: Dict[str,Any] = {}\n",
    "        for params in ParameterGrid(param_grid):\n",
    "            clf = clone(model).set_params(**params)\n",
    "            scores = []\n",
    "            for _, (itr, ivl) in enumerate(inner_cv.split(X_tr)):\n",
    "                X_itr, y_itr = X_tr.iloc[itr], y_tr.iloc[itr]\n",
    "                X_ivl, y_ivl = X_tr.iloc[ivl], y_tr.iloc[ivl]\n",
    "                clf.fit(X_itr, y_itr)\n",
    "                scores.append(scorer(clf, X_ivl, y_ivl))\n",
    "            mean_inner = sum(scores) / len(scores)\n",
    "            if mean_inner > best_score:\n",
    "                best_score = mean_inner\n",
    "                best_params = params\n",
    "\n",
    "        print(f\"üîç Best inner params fold {fold}: {best_params}, score {best_score:.4f}\")\n",
    "        best_params_list.append(best_params)\n",
    "\n",
    "        # Retrain on full train split\n",
    "        best_clf = clone(model).set_params(**best_params).fit(X_tr, y_tr)\n",
    "        outer_score = scorer(best_clf, X_te, y_te)\n",
    "        print(f\"‚úÖ Outer fold {fold} test {scoring}: {outer_score:.4f}\")\n",
    "        outer_scores.append(outer_score)\n",
    "\n",
    "        # Checkpoint each fold's model\n",
    "        ckpt_path = os.path.join(checkpoint_dir, f\"{prefix}_{model.__class__.__name__}_fold{fold}.pkl\")\n",
    "        with open(ckpt_path, 'wb') as f:\n",
    "            pickle.dump({'params': best_params}, f)\n",
    "\n",
    "    avg_score = sum(outer_scores) / len(outer_scores)\n",
    "    print(f\"üìä Average nested CV {scoring}: {avg_score:.4f}\")\n",
    "\n",
    "    # Choose most common best_params\n",
    "    from collections import Counter\n",
    "    param_tuples = [tuple(sorted(p.items())) for p in best_params_list]\n",
    "    most_common = Counter(param_tuples).most_common(1)[0][0]\n",
    "    final_params = dict(most_common)\n",
    "\n",
    "    # Final model on full data\n",
    "    final_model = clone(model).set_params(**final_params).fit(X, y)\n",
    "    return final_model, avg_score, outer_scores\n",
    "\n",
    "def train_multiple_models_with_split(\n",
    "    models: Dict[str, Tuple[BaseEstimator, Dict[str, Any]]],\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_splits: int = 5,\n",
    "    checkpoint_dir: str = 'checkpoints',\n",
    "    checkpoint_prefix: str = \"model\",\n",
    "    scoring: str = 'accuracy'\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    üìà Walk-forward (rolling) analysis:\n",
    "      - Split data into n_splits successive folds\n",
    "      - In each fold: grid-search on train window, test on next window\n",
    "      - Report average accuracy per model\n",
    "    \"\"\"\n",
    "    # Ensure checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare rolling splitter\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_results: Dict[str, List[float]] = {name: [] for name in models}\n",
    "\n",
    "    # Loop over each fold\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), start=1):\n",
    "        print(f\"\\nüìà Walk-forward fold {fold}/{n_splits}\")\n",
    "        X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[test_idx],  y.iloc[test_idx]\n",
    "\n",
    "        # For each model, tune then test\n",
    "        for name, (model, param_grid) in models.items():\n",
    "            print(f\"üîç Fold {fold} training {name}...\")\n",
    "            # Checkpoint per fold\n",
    "            ckpt = os.path.join(checkpoint_dir, f\"{checkpoint_prefix}_{name}_fold{fold}.pkl\")\n",
    "            best_model, _ = grid_search_with_checkpoint(\n",
    "                model=model,\n",
    "                param_grid=param_grid,\n",
    "                X=X_tr,\n",
    "                y=y_tr,\n",
    "                cv=TimeSeriesSplit(n_splits=3),\n",
    "                scoring=scoring,\n",
    "                checkpoint_path=ckpt,\n",
    "                resume=True\n",
    "            )\n",
    "            # Evaluate on validation slice\n",
    "            y_pred = best_model.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            print(f\"‚úÖ Fold {fold} {name} accuracy: {acc:.4f}\")\n",
    "            fold_results[name].append(acc)\n",
    "\n",
    "    # Compute & return average performance\n",
    "    avg_results: Dict[str, float] = {}\n",
    "    for name, scores in fold_results.items():\n",
    "        avg = sum(scores) / len(scores)\n",
    "        print(f\"\\nüìä {name} average walk-forward accuracy: {avg:.4f}\")\n",
    "        avg_results[name] = avg\n",
    "\n",
    "    return avg_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
